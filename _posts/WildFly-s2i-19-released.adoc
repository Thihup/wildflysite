---
layout: post
date:   2020-03-20
tags:   wildfly, galleon
author: jfdenise
---

= WildFly 19 S2I images have been released on quay.io

==  WildFly 19 S2I Docker images

The WildFly S2I (Source-to-Image) builder and runtime Docker images for WildFly 19 have been released on link:https://quay.io/organization/wildfly[quay.io/wildfly].

Changes since the link:https://wildfly.org/news/2019/10/07/WildFly-s2i-18-released/[last release]:

* Optimized startup. The server starts much faster, as the CLI script applied to the server configuration is now executed during server boot.
* Support for clustering. JGroups is now part of the default server configuration. 
* The WildFly 19 Galleon decorator layer `web-clustering` (support for Infinispan-based web session clustering) is fully usable in the openshift context.
This blog post highlights the new clustering features.
* The default standalone.xml configuration file includes the Microprofile JWT (JSON Web Token) subsystem.
* WildFly 19 defines three new Galleon layers for Microprofile 3.3 (`microprofile-fault-tolerance`, `microprofile-jwt` and `microprofile-openapi`) which can be referenced from the `GALLEON_PROVISION_LAYERS` env variable if you want Galleon to include those subsystems in a trimmed-down server it provisions.

For a complete documentation on how to use these images using S2I, OpenShift and Docker, 
refer to the WildFly S2I link:https://github.com/wildfly/wildfly-s2i/blob/wf-19.0/README.md[README].

== Adding the imagestreams and template to OpenShift

At some point the new images will be made available from the OpenShift catalog and image repository. But you can already use these images by adding them yourselves to your OpenShift cluster.

* WildFly S2I builder image stream: 
```
oc create -n myproject -f https://raw.githubusercontent.com/wildfly/wildfly-s2i/wf-19.0/imagestreams/wildfly-centos7.json
```
* WildFly runtime image stream: 
```
oc create -n myproject -f https://raw.githubusercontent.com/wildfly/wildfly-s2i/wf-19.0/imagestreams/wildfly-runtime-centos7.json
```
* Chained build template: 
```
oc create -n myproject -f https://raw.githubusercontent.com/wildfly/wildfly-s2i/wf-19.0/templates/wildfly-s2i-chained-build-template.yml
```

NB: If you import the image streams into your project, be sure to set the _ImageStreams Namespace_ (`IMAGE_STREAM_NAMESPACE` argument) to your project namespace in the template. _openshift_ being the default namespace.

== Clustering examples

=== Configure Openshift

* Allow to view all pods in the project: 
```
oc policy add-role-to-user view system:serviceaccount:$(oc project -q):default
```

* Be sure to kill any WildFly application running in the `myproject` namespace. Any running application would join the cluster.


=== Web session sharing using the `web-clustering` galleon layer

==== Build and run the application

We are provisioning a web server with support for web session sharing.

* Build the application image:
```
oc new-app wildfly-s2i-chained-build-template -p APPLICATION_NAME=web-clustering \
      -p GIT_REPO=https://github.com/wildfly/wildfly-s2i \
      -p GIT_CONTEXT_DIR=examples/web-clustering \
      -p GALLEON_PROVISION_LAYERS=web-server,web-clustering \
      -p IMAGE_STREAM_NAMESPACE=myproject
```

* Create an application from the application image:

```
oc new-app myproject/web-clustering -e KUBERNETES_NAMESPACE=myproject -e JGROUPS_CLUSTER_PASSWORD=mypassword
```

NB: The `KUBERNETES_NAMESPACE` is required to see other pods in the project, otherwise the server attempts to retrieve pods from the 'default' namespace which is not the one our project is using.
`JGROUPS_CLUSTER_PASSWORD` is used to authenticate servers in the cluster.

* Expose the service:
```
oc expose svc/web-clustering
```

* Access the application route, note the user created time and session ID.

* Scale the application to 2 pods: 
```
oc scale --replicas=2 dc web-clustering
```

* List pods: 
```
oc get pods
```

* Kill the oldest POD (which answered the first application request):
```
oc delete pod web-clustering-1-r4cx8 -n myproject
```

* Access the application again. You will notice that the displayed values are the same, meaning the web session has been shared between the 2 pods.

=== EJB singleton

Another aspect of the WildFly clustering features is the support for high availabilty EJB singletons. We are here running the `ha-singleton-deployment` WildFly quickstart.
In this example we are using the default server present in the wildfly S2I builder image; we are not provisioning a server trimmed-down with Galleon. We don't have (yet) layers
for EJB features.

* Build the application image:
```
oc new-app wildfly-s2i-chained-build-template -p APPLICATION_NAME=singleton \
      -p GIT_REPO=https://github.com/wildfly/quickstart/ \
      -p GIT_BRANCH=19.0.0.Final \
      -p GIT_CONTEXT_DIR=ha-singleton-deployment  \
      -p IMAGE_STREAM_NAMESPACE=myproject \
      --build-env=MAVEN_ARGS_APPEND=-Dcom.redhat.xpaas.repo.jbossorg
```

* Create an application from the application image:
```
oc new-app myproject/singleton -e KUBERNETES_NAMESPACE=myproject -e JGROUPS_CLUSTER_PASSWORD=mypassword
```

* Scale the application to 2 pods: 
```
oc scale --replicas=2 dc singleton
```

* Check the server logs: 
```
oc logs -f dc/singleton
```
One pod is elected for the timer service.

* List pods:
```
oc get pods
```

* Kill the oldest POD (elected one): 
```
oc delete pod singleton-1-r4cx8 -n myproject
```

The timer service is started in the remaining pod.
